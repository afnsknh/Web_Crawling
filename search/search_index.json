{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Afni Sakinah 160411100077 Web Mining Adalah proses menemukan hubungan intrinsic (misalkan, informasi yang menarik dan bermanfaat) dari data web, yang disajikan dalam bentuk teks, link, atau informasi penggunaan. Istilah web mining pertama kali digunakaan oleh Etzioni pada tahun 1996 (pakar data mining). Web mining dibagi menjadi tiga area pencarian utama yaitu web content mining , web structure mining , dan web usage mining . Referensi https://beritati.blogspot.com/2015/06/sekilas-tentang-web-mining-penambangan.html","title":"Home"},{"location":"#web-mining","text":"Adalah proses menemukan hubungan intrinsic (misalkan, informasi yang menarik dan bermanfaat) dari data web, yang disajikan dalam bentuk teks, link, atau informasi penggunaan. Istilah web mining pertama kali digunakaan oleh Etzioni pada tahun 1996 (pakar data mining). Web mining dibagi menjadi tiga area pencarian utama yaitu web content mining , web structure mining , dan web usage mining .","title":"Web Mining"},{"location":"#referensi","text":"https://beritati.blogspot.com/2015/06/sekilas-tentang-web-mining-penambangan.html","title":"Referensi"},{"location":"web_content/","text":"Web Content Mining Nama : Afni Sakinah NIM : 160411100077 Mata Kuliah : Penambangan dan Pencarian Web (Web ini dibuat untuk memenuhi tugas Ujian Tengah Semester mata kuliah Penambangan dan Pencarian Web serta untuk membagikan ilmu yang telah di dapat kepada khalayak umum) Definisi Web Crawler merupakan sebuah program atau script yang menggunakan metode tertentu yang memiliki tujuan untuk mengumpulkan secara otomatis semua data/informasi yang berada pada suatu website. Proses dari web crawler mengunjungi tiap dokumen dari website ini lah yang disebut dengan Web Crawling . Tujuan Tujuan utama dari web crawler adalah untuk mengumpulkan informasi-informasi yang ada dalam suatu halaman web. Contoh penggunaannya yang paling umum adalah search engine yang memiliki tujuan untuk menampilkan data/informasi yang relevan dengan permintaan atau keyword yang dicari oleh user . Sementara tujuan dari tugas ini adalah mengekstrak informasi dari website target kemudian melakukan preprocessing text yang kemudian akan di seleksi fitur dan pada akhirnya data akan di cluster menjadi beberapa kelompok. Tools dan Environment Python 3.6 Database Kamus Besar Bahasa Indonesia Library untuk Crawling : BeautifulSoup4 , requests Library untuk Preprocessing Text : Sastrawi Library untuk Seleksi Fitur dan Clustering : numpy , scipy , sklearn , skfuzzy Library untuk menyimpan data : sqlite3 (database), csv (import data ke csv) Website target : https://www.codepolitan.com/articles Data yang diambil adalah data artikel dari 3 pages, dimana tiap pages memiliki 16 artikel, jadi total data yang diambil adalah 48 data. Running pertama kali program membutuhkan koneksi internet untuk mengambil data dari website, jika sudah memiliki data yang di inginkan maka tidak perlu update data di awal. Crawling Data Proses pertama yang dilakukan adalah crawling data . Seperti yang sudah dijelaskan diatas, proses ini adalah proses pengambilan data dari website target. Code crawling data : Koneksi ke library import requests from bs4 import BeautifulSoup Penghubung ke website src = \"https://www.codepolitan.com/articles\" page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') Variabel penghubung ke html linkhead = soup.findAll(class_='block') pagination = soup.find(class_='pagination-wrapper') pagenum = pagination.findAll('a') Variabel linkhead untuk mengambil data dari kelas seluruh class block yang berada di page, variabel ini nantinya akan berguna sebagai dasar untuk pengambilan data tiap artikel Variabel pagination dan pagenum digunakan untuk mengambil data class pagination-wrapper yang akan digunakan untuk perpindahan menuju page selanjutnya. List penampung untuk crawling next page numpage = [] for i in pagination: numpage.append(i.getText()) List ini digunakan untuk menampung data data dari variabel pagination yang nantinya salah satu index datanya akan digunakan untuk perpindahan menuju page selanjutnya. Looping untuk mengambil data dalam tiap artikel for links in linkhead: try : src = links['href'] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find('article') title = konten.find(class_='type--bold').getText() author = konten.find('a').getText() temp = konten.findAll('p') isi = [] for j in range(len(temp)): isi += [temp[j].getText()] isif = \"\" for i in isi: isif += i Pada bagian ini pengambilan data dilakukan, dimana data yang diambil adalah judul artikel, pengarang artikel, dan isi dari artikel itu sendiri. Pengambilan data dilakukan dengan inspect elemen html dari page website target. Lalu ambil tag html yang sesuai dengan data yang diinginkan. Variabel konten merupakan parents data penampung dari tiap data. Variabel title dan author untuk mengambil data dari judul dan penulis. Variabel temp untuk mengambil data isi artikel. Dimana isi artikel memiliki banyak paragraf yang akan ditampung dalam list isi, lalu dijadikan kembali menjadi satu dalam variabel isif. Menyimpan Data ke Database Setelah berhasil melakukan crawling data dari website, maka data hasil crawling tersebut akan kita simpan ke dalam database. Code penyimpan data ke database : import sqlite3 conn = sqlite3.connect('datacoba.sqlite') conn.execute('''CREATE TABLE if not exists ARTIKEL (TITLE TEXT NOT NULL, AUTHOR TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.execute(\"INSERT INTO ARTIKEL (TITLE, AUTHOR, ISI) VALUES (?, ?, ?)\", (title, author, isif)) conn.commit() Import Data ke CSV Selain disimpan ke dalam database, data hasil crawling dan proses yang lain juga akan di import menjadi file csv agar lebih mudah di akses. Disini kita akan membuat fungsi tersendiri untuk import data ke dalam csv karena nantinya akan digunakan berulang kali. Berikut code untuk import ke csv : def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) Preprocessing Text Preprocessing text merupakan tahapan dimana data crawling di seleksi kembali atau di normalisasi agar tersusun menjadi data data yang lebih terstruktur untuk di analisi kembali pada tahap selanjutnya. Tahapan preprocessing text adalah sebagai berikut : Tokenisasi, merupakan tahap dimana tiap kata dalam data dipisah atau dipecah menjadi index-index tersendiri. Stopword, tahap setelah tokenisasi selesai dilakukan, yaitu proses seleksi kata-kata hubung atau tanda baca yang tidak diperlukan dari tiap kata. Stemming, tahap penyusutan kata dengan cara mengambil nilai kata dasar dari tiap kata. Tambahan proses seleksi data dengan menggunakan database Kamus Besar Bahasa Indonesia dimana kata yang telah melalui proses stemming akan dicocokkan dengan data yang berada dalam database KBBI, jika kata tersebut ada maka akan disimpan, dan jika tidak maka akan dibuang. Code untuk preprocessing text : # preprocessing text factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() stop = stopword.remove(isif) stem = stemmer.stem(stop) katadasar = stem.split() matrix=[] for row in cursor: tampung = [] for i in katadasar: tampung.append(row[2].lower().count(i)) matrix.append(tampung) print(katadasar) write_csv(\"kata_before_%s.csv\"%n, katadasar) #Sharing kata Sesuai KBI Belum VSM conn = sqlite3.connect('KBI.db') cur_kbi = conn.execute(\"SELECT* from KATA\") # fungsi cek kata dengan KBBI def LinearSearch (kbi,kata): found=False posisi=0 while posisi < len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] berhasil2='' for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) berhasil2=berhasil2+' '+kata print(berhasil) #import csv kata yg sesuai dengan KBI write_csv(\"kata_after_%s.csv\"%n, berhasil) TF-IDF TF-IDF merupakan dua statistik data yang terdiri dari Term-Frequency dan Inverse Data-Frequency . TF(Term-Frequency) Merupakan data statistik yang berisi frekuensi kemunculan dari tiap kata (fitur) terhadap tiap data. data 1 : \"saya makan nasi\" data 2 : \"nasi putih di makan saya\" data saya makan nasi putih di 1 1 1 0 0 0 2 1 1 1 1 1 Code untuk TF : # VSM setelah cek KBI conn = sqlite3.connect('datacoba.sqlite') matrix2=[] cursor = conn.execute(\"SELECT* from ARTIKEL\") for row in cursor: tampung = [] for i in berhasil: tampung.append(row[2].lower().count(i)) matrix2.append(tampung) print(matrix2) IDF(Inverse Data-Frequency) Data statistik yang berisi hasil inverse dari Data-Frequency (DF) dimana DF didapatkan dari nilai kemunculan tiap kata (fitur) berapa kali dalam semua data. data 1 : \"saya makan nasi\" data 2 : \"nasi putih di makan saya\" kata jumlah data yang mengandung kata saya 2 makan 2 nasi 2 putih 1 di 1 Code untuk DF : df = list() for d in range (len(matrix2[0])): total = 0 for i in range(len(matrix2)): if matrix2[i][d] !=0: total += 1 df.append(total) Setelah mendapatkan nilai DF maka yang selanjutnya dilakukan adalah menghitung nilai invers nya ( IDF ). Code untuk menghitung IDF : idf = list() for i in df: tmp = 1 + log10(len(matrix2)/(1+i)) idf.append(tmp) tf = matrix2 tfidf = [] for baris in range(len(matrix2)): tampungBaris = [] for kolom in range(len(matrix2[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) # import csv data tdidf write_csv(\"tfidf_%s.csv\"%n, tfidf) Seleksi Fitur Proses reduksi dimensi dari fitur yang ada dengan menggunakan suatu tolak ukur untuk menentukan sesuai atau tidaknya fitur tersebut dengan kebutuhan proses analisis selanjutnya. Seleksi fitur memiliki banyak metode, salah satunya adalah Pearson Correlation Coefficient (PCC) yang akan kita gunakan dalam tugas kali ini. PCC menyeleksi fitur menggunakan perbandingan nilai koefisien antar tiap dua data. Semakin tinggi nilai koefisien korelasi fitur dengan koefisien batas yang ditentukan maka semakin kuat hubungan fiturny, begitu pula sebaliknya, semakin kecil nilai koefisien korelasi maka semakin lemah hubungan fiturnya. Fitur yang memiliki korelasi paling lemah akan dibuang/di drop dari data yang ada. Code untuk Pearson Selection : def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(data, threshold, berhasil): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] seleksikata=berhasil[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) seleksikata = np.hstack((seleksikata, berhasil[v])) v+=1 data = dataBaru meanFitur=meanBaru berhasil=seleksikata if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data, seleksikata xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil) xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil) # import csv data setelah di seleksi fitur write_csv(\"kata_pearson_%s.csv\"%n, kataBaru2) Clustering Merupakan proses pengelompokan data yang memiliki karakteristik sama menjadi satu kelompok yang sama. Metode untuk clustering ada banyak sekali. Namun kali ini kita akan menggunakan metode Fuzzy C-Mean . Dalam proses ini kita akan meng clusterisasi data menggunakan library dari python yaitu sklearn dan skfuzzy. Code untuk clustering : print(\"Cluster dgn Seleksi Fitur : 0.8\") cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) fuzz.cmeans memiliki beberapa parameter utama, yang pertama parameter untuk menampung data matriks yang akan di cluster, parameter kedua adalah jumlah cluster yang ingin dibagi, yang ketiga adalah nilai m atau pembobot, parameter keempat untuk mengatur nilai error maksimal, lalu parameter kelima untuk mengatur jumlah maksimal iterasi. Setelah tiap data di cluster maka langkah selanjutnya adalah menghitung nilai silhouette. Penggunaan metode ini untuk mengukur seberapa dekat relasi antar objek dalam tiap cluster. Code untuk menghitung silhouette dan output hasil perhitungan silhouette tiap objek : ``` silhouette = silhouette_samples(xBaru1, membership) s_avg = silhouette_score(xBaru1, membership, random_state=10) for i in range(len(tfidf)): print(\"c \"+str(membership[i]))#+\"\\t\" + str(silhouette[i])) print(s_avg) ``` Kesimpulan Karena pada program ini hanya dilakukan percobaan satu metode seleksi fitur dan satu metode clustering saja jadi data yang ada menunjukkan bahwa dengan menggunakan seleksi fitur Pearson Correlation Coefficient (PCC) lalu data di cluster dengan menggunakan metode Fuzzy C-Mean merupakan hasil yang belum sempurna menurut penulis, masih diperlukan banyak percobaan dan perbandingan lagi menggunakan metode lain yang kemungkinan besar bisa menghasilkan akurasi hasil cluster data yang lebih akurat. Referensi https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3 https://pintasku.com/tutorial/mengenal-dan-memahami-apa-itu-web-crawler/ https://aprianapanca.wordpress.com/2017/09/22/penerapan-tokenisasi-stopword-removal-dan-stemming/ http://taufiksutanto.blogspot.com/2018/01/tokenization-dalam-bahasa-inggris.html https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/ https://devtrik.com/python/steeming-bahasa-indonesia-python-sastrawi/ https://informatikalogi.com/term-weighting-tf-idf/ https://rahmadya.com/2014/09/25/term-frequency-dan-invers-document-frequency-tf-idf/ https://wahyudisetiawan.wordpress.com/tag/seleksi-fitur/ https://en.wikipedia.org/wiki/Pearson_correlation_coefficient https://yudiagusta.wordpress.com/clustering/ https://en.wikipedia.org/wiki/Fuzzy_clustering https://home.deib.polimi.it/matteucc/Clustering/tutorial_html/cmeans.html http://blog.farifam.com/2018/02/26/rstudio-menentukan-n-cluster-dengan-nilai-silhouette/ http://nopi-en.blogspot.com/2018/11/pengujian-silhouette-coefficient.html","title":"Web Content Mining"},{"location":"web_content/#web-content-mining","text":"Nama : Afni Sakinah NIM : 160411100077 Mata Kuliah : Penambangan dan Pencarian Web (Web ini dibuat untuk memenuhi tugas Ujian Tengah Semester mata kuliah Penambangan dan Pencarian Web serta untuk membagikan ilmu yang telah di dapat kepada khalayak umum)","title":"Web Content Mining"},{"location":"web_content/#definisi","text":"Web Crawler merupakan sebuah program atau script yang menggunakan metode tertentu yang memiliki tujuan untuk mengumpulkan secara otomatis semua data/informasi yang berada pada suatu website. Proses dari web crawler mengunjungi tiap dokumen dari website ini lah yang disebut dengan Web Crawling .","title":"Definisi"},{"location":"web_content/#tujuan","text":"Tujuan utama dari web crawler adalah untuk mengumpulkan informasi-informasi yang ada dalam suatu halaman web. Contoh penggunaannya yang paling umum adalah search engine yang memiliki tujuan untuk menampilkan data/informasi yang relevan dengan permintaan atau keyword yang dicari oleh user . Sementara tujuan dari tugas ini adalah mengekstrak informasi dari website target kemudian melakukan preprocessing text yang kemudian akan di seleksi fitur dan pada akhirnya data akan di cluster menjadi beberapa kelompok.","title":"Tujuan"},{"location":"web_content/#tools-dan-environment","text":"Python 3.6 Database Kamus Besar Bahasa Indonesia Library untuk Crawling : BeautifulSoup4 , requests Library untuk Preprocessing Text : Sastrawi Library untuk Seleksi Fitur dan Clustering : numpy , scipy , sklearn , skfuzzy Library untuk menyimpan data : sqlite3 (database), csv (import data ke csv) Website target : https://www.codepolitan.com/articles Data yang diambil adalah data artikel dari 3 pages, dimana tiap pages memiliki 16 artikel, jadi total data yang diambil adalah 48 data. Running pertama kali program membutuhkan koneksi internet untuk mengambil data dari website, jika sudah memiliki data yang di inginkan maka tidak perlu update data di awal.","title":"Tools dan Environment"},{"location":"web_content/#crawling-data","text":"Proses pertama yang dilakukan adalah crawling data . Seperti yang sudah dijelaskan diatas, proses ini adalah proses pengambilan data dari website target. Code crawling data : Koneksi ke library import requests from bs4 import BeautifulSoup Penghubung ke website src = \"https://www.codepolitan.com/articles\" page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') Variabel penghubung ke html linkhead = soup.findAll(class_='block') pagination = soup.find(class_='pagination-wrapper') pagenum = pagination.findAll('a') Variabel linkhead untuk mengambil data dari kelas seluruh class block yang berada di page, variabel ini nantinya akan berguna sebagai dasar untuk pengambilan data tiap artikel Variabel pagination dan pagenum digunakan untuk mengambil data class pagination-wrapper yang akan digunakan untuk perpindahan menuju page selanjutnya. List penampung untuk crawling next page numpage = [] for i in pagination: numpage.append(i.getText()) List ini digunakan untuk menampung data data dari variabel pagination yang nantinya salah satu index datanya akan digunakan untuk perpindahan menuju page selanjutnya. Looping untuk mengambil data dalam tiap artikel for links in linkhead: try : src = links['href'] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find('article') title = konten.find(class_='type--bold').getText() author = konten.find('a').getText() temp = konten.findAll('p') isi = [] for j in range(len(temp)): isi += [temp[j].getText()] isif = \"\" for i in isi: isif += i Pada bagian ini pengambilan data dilakukan, dimana data yang diambil adalah judul artikel, pengarang artikel, dan isi dari artikel itu sendiri. Pengambilan data dilakukan dengan inspect elemen html dari page website target. Lalu ambil tag html yang sesuai dengan data yang diinginkan. Variabel konten merupakan parents data penampung dari tiap data. Variabel title dan author untuk mengambil data dari judul dan penulis. Variabel temp untuk mengambil data isi artikel. Dimana isi artikel memiliki banyak paragraf yang akan ditampung dalam list isi, lalu dijadikan kembali menjadi satu dalam variabel isif.","title":"Crawling Data"},{"location":"web_content/#menyimpan-data-ke-database","text":"Setelah berhasil melakukan crawling data dari website, maka data hasil crawling tersebut akan kita simpan ke dalam database. Code penyimpan data ke database : import sqlite3 conn = sqlite3.connect('datacoba.sqlite') conn.execute('''CREATE TABLE if not exists ARTIKEL (TITLE TEXT NOT NULL, AUTHOR TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.execute(\"INSERT INTO ARTIKEL (TITLE, AUTHOR, ISI) VALUES (?, ?, ?)\", (title, author, isif)) conn.commit()","title":"Menyimpan Data ke Database"},{"location":"web_content/#import-data-ke-csv","text":"Selain disimpan ke dalam database, data hasil crawling dan proses yang lain juga akan di import menjadi file csv agar lebih mudah di akses. Disini kita akan membuat fungsi tersendiri untuk import data ke dalam csv karena nantinya akan digunakan berulang kali. Berikut code untuk import ke csv : def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row)","title":"Import Data ke CSV"},{"location":"web_content/#preprocessing-text","text":"Preprocessing text merupakan tahapan dimana data crawling di seleksi kembali atau di normalisasi agar tersusun menjadi data data yang lebih terstruktur untuk di analisi kembali pada tahap selanjutnya. Tahapan preprocessing text adalah sebagai berikut : Tokenisasi, merupakan tahap dimana tiap kata dalam data dipisah atau dipecah menjadi index-index tersendiri. Stopword, tahap setelah tokenisasi selesai dilakukan, yaitu proses seleksi kata-kata hubung atau tanda baca yang tidak diperlukan dari tiap kata. Stemming, tahap penyusutan kata dengan cara mengambil nilai kata dasar dari tiap kata. Tambahan proses seleksi data dengan menggunakan database Kamus Besar Bahasa Indonesia dimana kata yang telah melalui proses stemming akan dicocokkan dengan data yang berada dalam database KBBI, jika kata tersebut ada maka akan disimpan, dan jika tidak maka akan dibuang. Code untuk preprocessing text : # preprocessing text factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() stop = stopword.remove(isif) stem = stemmer.stem(stop) katadasar = stem.split() matrix=[] for row in cursor: tampung = [] for i in katadasar: tampung.append(row[2].lower().count(i)) matrix.append(tampung) print(katadasar) write_csv(\"kata_before_%s.csv\"%n, katadasar) #Sharing kata Sesuai KBI Belum VSM conn = sqlite3.connect('KBI.db') cur_kbi = conn.execute(\"SELECT* from KATA\") # fungsi cek kata dengan KBBI def LinearSearch (kbi,kata): found=False posisi=0 while posisi < len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] berhasil2='' for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) berhasil2=berhasil2+' '+kata print(berhasil) #import csv kata yg sesuai dengan KBI write_csv(\"kata_after_%s.csv\"%n, berhasil)","title":"Preprocessing Text"},{"location":"web_content/#tf-idf","text":"TF-IDF merupakan dua statistik data yang terdiri dari Term-Frequency dan Inverse Data-Frequency .","title":"TF-IDF"},{"location":"web_content/#tfterm-frequency","text":"Merupakan data statistik yang berisi frekuensi kemunculan dari tiap kata (fitur) terhadap tiap data. data 1 : \"saya makan nasi\" data 2 : \"nasi putih di makan saya\" data saya makan nasi putih di 1 1 1 0 0 0 2 1 1 1 1 1 Code untuk TF : # VSM setelah cek KBI conn = sqlite3.connect('datacoba.sqlite') matrix2=[] cursor = conn.execute(\"SELECT* from ARTIKEL\") for row in cursor: tampung = [] for i in berhasil: tampung.append(row[2].lower().count(i)) matrix2.append(tampung) print(matrix2)","title":"TF(Term-Frequency)"},{"location":"web_content/#idfinverse-data-frequency","text":"Data statistik yang berisi hasil inverse dari Data-Frequency (DF) dimana DF didapatkan dari nilai kemunculan tiap kata (fitur) berapa kali dalam semua data. data 1 : \"saya makan nasi\" data 2 : \"nasi putih di makan saya\" kata jumlah data yang mengandung kata saya 2 makan 2 nasi 2 putih 1 di 1 Code untuk DF : df = list() for d in range (len(matrix2[0])): total = 0 for i in range(len(matrix2)): if matrix2[i][d] !=0: total += 1 df.append(total) Setelah mendapatkan nilai DF maka yang selanjutnya dilakukan adalah menghitung nilai invers nya ( IDF ). Code untuk menghitung IDF : idf = list() for i in df: tmp = 1 + log10(len(matrix2)/(1+i)) idf.append(tmp) tf = matrix2 tfidf = [] for baris in range(len(matrix2)): tampungBaris = [] for kolom in range(len(matrix2[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) # import csv data tdidf write_csv(\"tfidf_%s.csv\"%n, tfidf)","title":"IDF(Inverse Data-Frequency)"},{"location":"web_content/#seleksi-fitur","text":"Proses reduksi dimensi dari fitur yang ada dengan menggunakan suatu tolak ukur untuk menentukan sesuai atau tidaknya fitur tersebut dengan kebutuhan proses analisis selanjutnya. Seleksi fitur memiliki banyak metode, salah satunya adalah Pearson Correlation Coefficient (PCC) yang akan kita gunakan dalam tugas kali ini. PCC menyeleksi fitur menggunakan perbandingan nilai koefisien antar tiap dua data. Semakin tinggi nilai koefisien korelasi fitur dengan koefisien batas yang ditentukan maka semakin kuat hubungan fiturny, begitu pula sebaliknya, semakin kecil nilai koefisien korelasi maka semakin lemah hubungan fiturnya. Fitur yang memiliki korelasi paling lemah akan dibuang/di drop dari data yang ada. Code untuk Pearson Selection : def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(data, threshold, berhasil): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] seleksikata=berhasil[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) seleksikata = np.hstack((seleksikata, berhasil[v])) v+=1 data = dataBaru meanFitur=meanBaru berhasil=seleksikata if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data, seleksikata xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil) xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil) # import csv data setelah di seleksi fitur write_csv(\"kata_pearson_%s.csv\"%n, kataBaru2)","title":"Seleksi Fitur"},{"location":"web_content/#clustering","text":"Merupakan proses pengelompokan data yang memiliki karakteristik sama menjadi satu kelompok yang sama. Metode untuk clustering ada banyak sekali. Namun kali ini kita akan menggunakan metode Fuzzy C-Mean . Dalam proses ini kita akan meng clusterisasi data menggunakan library dari python yaitu sklearn dan skfuzzy. Code untuk clustering : print(\"Cluster dgn Seleksi Fitur : 0.8\") cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) fuzz.cmeans memiliki beberapa parameter utama, yang pertama parameter untuk menampung data matriks yang akan di cluster, parameter kedua adalah jumlah cluster yang ingin dibagi, yang ketiga adalah nilai m atau pembobot, parameter keempat untuk mengatur nilai error maksimal, lalu parameter kelima untuk mengatur jumlah maksimal iterasi. Setelah tiap data di cluster maka langkah selanjutnya adalah menghitung nilai silhouette. Penggunaan metode ini untuk mengukur seberapa dekat relasi antar objek dalam tiap cluster. Code untuk menghitung silhouette dan output hasil perhitungan silhouette tiap objek : ``` silhouette = silhouette_samples(xBaru1, membership) s_avg = silhouette_score(xBaru1, membership, random_state=10) for i in range(len(tfidf)): print(\"c \"+str(membership[i]))#+\"\\t\" + str(silhouette[i])) print(s_avg) ```","title":"Clustering"},{"location":"web_content/#kesimpulan","text":"Karena pada program ini hanya dilakukan percobaan satu metode seleksi fitur dan satu metode clustering saja jadi data yang ada menunjukkan bahwa dengan menggunakan seleksi fitur Pearson Correlation Coefficient (PCC) lalu data di cluster dengan menggunakan metode Fuzzy C-Mean merupakan hasil yang belum sempurna menurut penulis, masih diperlukan banyak percobaan dan perbandingan lagi menggunakan metode lain yang kemungkinan besar bisa menghasilkan akurasi hasil cluster data yang lebih akurat.","title":"Kesimpulan"},{"location":"web_content/#referensi","text":"https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3 https://pintasku.com/tutorial/mengenal-dan-memahami-apa-itu-web-crawler/ https://aprianapanca.wordpress.com/2017/09/22/penerapan-tokenisasi-stopword-removal-dan-stemming/ http://taufiksutanto.blogspot.com/2018/01/tokenization-dalam-bahasa-inggris.html https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/ https://devtrik.com/python/steeming-bahasa-indonesia-python-sastrawi/ https://informatikalogi.com/term-weighting-tf-idf/ https://rahmadya.com/2014/09/25/term-frequency-dan-invers-document-frequency-tf-idf/ https://wahyudisetiawan.wordpress.com/tag/seleksi-fitur/ https://en.wikipedia.org/wiki/Pearson_correlation_coefficient https://yudiagusta.wordpress.com/clustering/ https://en.wikipedia.org/wiki/Fuzzy_clustering https://home.deib.polimi.it/matteucc/Clustering/tutorial_html/cmeans.html http://blog.farifam.com/2018/02/26/rstudio-menentukan-n-cluster-dengan-nilai-silhouette/ http://nopi-en.blogspot.com/2018/11/pengujian-silhouette-coefficient.html","title":"Referensi"},{"location":"web_strct/","text":"Web Structure Mining Nama : Afni Sakinah NIM : 160411100077 Mata Kuliah : Penambangan dan Pencarian Web (Web ini dibuat untuk memenuhi tugas Ujian Tengah Semester mata kuliah Penambangan dan Pencarian Web serta untuk membagikan ilmu yang telah di dapat kepada khalayak umum) Definisi Web structure mining dikenal juga sebagai web log mining adalah teknik yang digunakan untuk menemukan struktur link dari hyperlink dan membangun rangkuman website dan halaman web. Tujuan Salah satu tujuan dari web structure mining adalah mengkategorikan website menurut rangkuman struktur link yang telah didapat berdasarkan kemiripan dan seberapa banyak website itu diakses oleh website yang lain. Tools dan Environment Python 3.6 Library untuk Crawling : BeautifulSoup4 , requests Library untuk Preprocessing Text : Sastrawi Library untuk Menampung dan Menghitung Pagerank : pa , networkx Library untuk Menampilkan Graph : matplotlib Website target : https://www.codepolitan.com/articles Data yang diambil adalah struktural data dalam website yang memiliki hyperlink di dalam tag nya Crawling Data Proses pertama yang dilakukan adalah crawling data . Proses ini adalah proses pengambilan data dari website target. Code crawling data : Koneksi ke library import requests from bs4 import BeautifulSoup Fungsi mengambil data def getLink(src): try: page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') a = soup.findAll('a') temp = [] for i in a : try: link = i['href'] if not link in temp and 'http' in link : temp.append(link) except KeyError: pass return temp print(temp) except: return list() Pada web structure mining, data yang akan di ambil adalah data yang memiliki hyperlink di dalam tag nya. Di html, link biasa ditampung dalam tag a dengan atribut href. Maka dari itu yang pertama dilakukan adalah mengambil seluruh data yang memiliki tag a di dalamnya, kemudian melakukan filter dengan mengambil tag a yang memiliki atribut href dan mengambil isi dari atribut tersebut untuk ditampung ke dalam list. Fungsi filter link def simplifiedURL(url): #kondisi pertama if \"www.\" in url: ind = url.index(\"www.\")+4 url = \"http://\"+url[ind:] #kondisi kedua if url[-1] == \"/\": url = url[:-1] #kondisi ketiga parts = url.split(\"/\") url = '' for i in range(3): url += parts[i] + \"/\" return url Setelah mendapatkan data berupa hyperlink-hyperlink yang berada di dalam website, maka selanjutnya akan dilakukan filter terhadap data link yang dimiliki agar memastikan bahwa link yang diambil benar. Terdapat beberapa kondisi untuk filtering. Kondisi yang pertama untuk menyamaratakan link menjadi berstruktur http. Lalu kondisi kedua mengecek keberadaan garis miring setelah link , jika ada maka link akan diambil dari awal hingga posisi terakhir tersebut. Dan kondisi terakhir adalah kondisi untuk mengambil link utama dari website. Contoh kondisi pertama www.codepolitan.com menjadi : http://codepolitan.com/ Contoh kondisi ketiga http://codepolitan.com/articles/ menjadi : http://codepolitan.com/ Fungsi Crawl def crawl(url, max_deep, show=False, deep=0, done=[]): global edgelist url = simplifiedURL(url) deep += 1 if not url in done: links = getLink(url) done.append(url) if show: if deep == 1: print(url) else: print(\"|\", end=\"\") for i in range(deep-1): print(\"--\", end=\"\") print(\"(%d)%s\" %(len(links),url)) for link in links: link = simplifiedURL(link) edge = (url,link) if not edge in edgelist: edgelist.append(edge) if (deep != max_deep): crawl(link, max_deep, show, deep) Kedua fungsi diatas akan digabungkan dengan memanggilnya di dalam fungsi crawl. Dimana fungsi ini nantinya akan dipanggil lagi pada main program. root = \"https://www.codepolitan.com/articles\" edgelist = [] nodelist = [root] tampil = True crawl(root, 3, show=tampil) edgeListFrame = pd.DataFrame(edgelist, None, (\"From\", \"To\")) Pagerank Pagerank merupakan suatu algoritma yang diciptakan oleh founder Google (Larry Page dan Sergey Brin) untuk memberikan penilaian seberapa tinggi score dari sebuah halaman website. Semakin tinggi score halaman website tersebut, maka artinya semakin bagus pula kualitas dari website tersebut. Penghitungan pagerank pada dasarnya sangat sederhana, dimana semakin banyak link yang merujuk pada suatu website, maka nilai pagerank dari website tersebut akan semakin meningkat. Untuk menghitung pagerank digunakan code sebagai berikut : damping = 0.85 max_iterr = 100 error_toleransi = 0.01 pr = nx.pagerank(g, alpha = damping, max_iter = max_iterr, tol = error_toleransi) Penghitungan pagerank bisa dilakukan dengan library dari python yaitu networkx , dalam library ini terdapat module pagerank yang memiliki beberapa parameter : g merupakan variabel penampung graph. alpha berisi nilai damping untuk penghitungan yang biasanya bernilai default 0.85. max_iter untuk nilai maksimum iterasi yang diizinkan. tol untuk menampung nilai minimal error toleransi yang diizinkan. Kemudian yang dilakukan adalah menampung hasil perhitungan ke dalam list. Berikut adalah list code nya : print(\"\\n keterangan node:\") nodelist = g.nodes label= {} data = [] for i, key in enumerate(nodelist): data.append((pr[key], key)) label[key]=i Setelah memiliki tampungan list dan nilai pagerank dalam variabel data, maka sekarang akan dilakukan pengurutan nilai pagerank dari list yang telah tertampung urut = data.copy() for x in range(len(urut)): for y in range(len(urut)): if urut[x][0] > urut[y][0]: urut[x], urut[y] = urut[y], urut[x] urut = pd.DataFrame(data, None, (\"Pagerank\", \"Node\")) print(urut) Hasil dari 5 besar nilai pagerank tertinggi : No Nilai Pagerank Node 0 0.008815 http://codepolitan.com/ 1 0.008810 https://devschool.id/ 2 0.008766 http://kadinjabar.or.id/ 3 0.008705 http://essentia.id/ 4 0.00876 http://gokad.com/ Graph Graph digunakan untuk mendapatkan visualisasi yang lebih jelas dan terstruktur dari hasil pagerank yang telah didapat. Code untuk membentuk graph : g = nx.from_pandas_edgelist(edgeListFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.spring_layout(g) plt.title('graph codepolitan') nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color=\"w\") plt.axis(\"off\") plt.show() Hasil graph Referensi https://sis.binus.ac.id/2016/12/15/teori-text-mining-dan-web-mining/ http://www.cyberartsweb.org/cpace/ht/lanman/wsm1.htm https://id.wikipedia.org/wiki/PageRank http://www.klikseo.com/apa-itu-pagerank-dan-kaitannya-dengan-seo/","title":"Web Structure Mining"},{"location":"web_strct/#web-structure-mining","text":"Nama : Afni Sakinah NIM : 160411100077 Mata Kuliah : Penambangan dan Pencarian Web (Web ini dibuat untuk memenuhi tugas Ujian Tengah Semester mata kuliah Penambangan dan Pencarian Web serta untuk membagikan ilmu yang telah di dapat kepada khalayak umum)","title":"Web Structure Mining"},{"location":"web_strct/#definisi","text":"Web structure mining dikenal juga sebagai web log mining adalah teknik yang digunakan untuk menemukan struktur link dari hyperlink dan membangun rangkuman website dan halaman web.","title":"Definisi"},{"location":"web_strct/#tujuan","text":"Salah satu tujuan dari web structure mining adalah mengkategorikan website menurut rangkuman struktur link yang telah didapat berdasarkan kemiripan dan seberapa banyak website itu diakses oleh website yang lain.","title":"Tujuan"},{"location":"web_strct/#tools-dan-environment","text":"Python 3.6 Library untuk Crawling : BeautifulSoup4 , requests Library untuk Preprocessing Text : Sastrawi Library untuk Menampung dan Menghitung Pagerank : pa , networkx Library untuk Menampilkan Graph : matplotlib Website target : https://www.codepolitan.com/articles Data yang diambil adalah struktural data dalam website yang memiliki hyperlink di dalam tag nya","title":"Tools dan Environment"},{"location":"web_strct/#crawling-data","text":"Proses pertama yang dilakukan adalah crawling data . Proses ini adalah proses pengambilan data dari website target. Code crawling data : Koneksi ke library import requests from bs4 import BeautifulSoup Fungsi mengambil data def getLink(src): try: page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') a = soup.findAll('a') temp = [] for i in a : try: link = i['href'] if not link in temp and 'http' in link : temp.append(link) except KeyError: pass return temp print(temp) except: return list() Pada web structure mining, data yang akan di ambil adalah data yang memiliki hyperlink di dalam tag nya. Di html, link biasa ditampung dalam tag a dengan atribut href. Maka dari itu yang pertama dilakukan adalah mengambil seluruh data yang memiliki tag a di dalamnya, kemudian melakukan filter dengan mengambil tag a yang memiliki atribut href dan mengambil isi dari atribut tersebut untuk ditampung ke dalam list. Fungsi filter link def simplifiedURL(url): #kondisi pertama if \"www.\" in url: ind = url.index(\"www.\")+4 url = \"http://\"+url[ind:] #kondisi kedua if url[-1] == \"/\": url = url[:-1] #kondisi ketiga parts = url.split(\"/\") url = '' for i in range(3): url += parts[i] + \"/\" return url Setelah mendapatkan data berupa hyperlink-hyperlink yang berada di dalam website, maka selanjutnya akan dilakukan filter terhadap data link yang dimiliki agar memastikan bahwa link yang diambil benar. Terdapat beberapa kondisi untuk filtering. Kondisi yang pertama untuk menyamaratakan link menjadi berstruktur http. Lalu kondisi kedua mengecek keberadaan garis miring setelah link , jika ada maka link akan diambil dari awal hingga posisi terakhir tersebut. Dan kondisi terakhir adalah kondisi untuk mengambil link utama dari website. Contoh kondisi pertama www.codepolitan.com menjadi : http://codepolitan.com/ Contoh kondisi ketiga http://codepolitan.com/articles/ menjadi : http://codepolitan.com/ Fungsi Crawl def crawl(url, max_deep, show=False, deep=0, done=[]): global edgelist url = simplifiedURL(url) deep += 1 if not url in done: links = getLink(url) done.append(url) if show: if deep == 1: print(url) else: print(\"|\", end=\"\") for i in range(deep-1): print(\"--\", end=\"\") print(\"(%d)%s\" %(len(links),url)) for link in links: link = simplifiedURL(link) edge = (url,link) if not edge in edgelist: edgelist.append(edge) if (deep != max_deep): crawl(link, max_deep, show, deep) Kedua fungsi diatas akan digabungkan dengan memanggilnya di dalam fungsi crawl. Dimana fungsi ini nantinya akan dipanggil lagi pada main program. root = \"https://www.codepolitan.com/articles\" edgelist = [] nodelist = [root] tampil = True crawl(root, 3, show=tampil) edgeListFrame = pd.DataFrame(edgelist, None, (\"From\", \"To\"))","title":"Crawling Data"},{"location":"web_strct/#pagerank","text":"Pagerank merupakan suatu algoritma yang diciptakan oleh founder Google (Larry Page dan Sergey Brin) untuk memberikan penilaian seberapa tinggi score dari sebuah halaman website. Semakin tinggi score halaman website tersebut, maka artinya semakin bagus pula kualitas dari website tersebut. Penghitungan pagerank pada dasarnya sangat sederhana, dimana semakin banyak link yang merujuk pada suatu website, maka nilai pagerank dari website tersebut akan semakin meningkat. Untuk menghitung pagerank digunakan code sebagai berikut : damping = 0.85 max_iterr = 100 error_toleransi = 0.01 pr = nx.pagerank(g, alpha = damping, max_iter = max_iterr, tol = error_toleransi) Penghitungan pagerank bisa dilakukan dengan library dari python yaitu networkx , dalam library ini terdapat module pagerank yang memiliki beberapa parameter : g merupakan variabel penampung graph. alpha berisi nilai damping untuk penghitungan yang biasanya bernilai default 0.85. max_iter untuk nilai maksimum iterasi yang diizinkan. tol untuk menampung nilai minimal error toleransi yang diizinkan. Kemudian yang dilakukan adalah menampung hasil perhitungan ke dalam list. Berikut adalah list code nya : print(\"\\n keterangan node:\") nodelist = g.nodes label= {} data = [] for i, key in enumerate(nodelist): data.append((pr[key], key)) label[key]=i Setelah memiliki tampungan list dan nilai pagerank dalam variabel data, maka sekarang akan dilakukan pengurutan nilai pagerank dari list yang telah tertampung urut = data.copy() for x in range(len(urut)): for y in range(len(urut)): if urut[x][0] > urut[y][0]: urut[x], urut[y] = urut[y], urut[x] urut = pd.DataFrame(data, None, (\"Pagerank\", \"Node\")) print(urut) Hasil dari 5 besar nilai pagerank tertinggi : No Nilai Pagerank Node 0 0.008815 http://codepolitan.com/ 1 0.008810 https://devschool.id/ 2 0.008766 http://kadinjabar.or.id/ 3 0.008705 http://essentia.id/ 4 0.00876 http://gokad.com/","title":"Pagerank"},{"location":"web_strct/#graph","text":"Graph digunakan untuk mendapatkan visualisasi yang lebih jelas dan terstruktur dari hasil pagerank yang telah didapat. Code untuk membentuk graph : g = nx.from_pandas_edgelist(edgeListFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.spring_layout(g) plt.title('graph codepolitan') nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color=\"w\") plt.axis(\"off\") plt.show() Hasil graph","title":"Graph"},{"location":"web_strct/#referensi","text":"https://sis.binus.ac.id/2016/12/15/teori-text-mining-dan-web-mining/ http://www.cyberartsweb.org/cpace/ht/lanman/wsm1.htm https://id.wikipedia.org/wiki/PageRank http://www.klikseo.com/apa-itu-pagerank-dan-kaitannya-dengan-seo/","title":"Referensi"}]}