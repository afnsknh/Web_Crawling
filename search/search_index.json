{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Web Crawling Nama : Afni Sakinah NIM : 160411100077 Mata Kuliah : Penambangan dan Pencarian Web (Web ini dibuat untuk memenuhi tugas Ujian Tengah Semester mata kuliah Penambangan dan Pencarian Web serta untuk membagikan ilmu yang telah di dapat kepada khalayak umum) Definisi Web Crawler merupakan sebuah program atau script yang menggunakan metode tertentu yang memiliki tujuan untuk mengumpulkan secara otomatis semua data/informasi yang berada pada suatu website. Proses dari web crawler mengunjungi tiap dokumen dari website ini lah yang disebut dengan Web Crawling . Tujuan Tujuan utama dari web crawler adalah untuk mengumpulkan informasi-informasi yang ada dalam suatu halaman web. Contoh penggunaannya yang paling umum adalah search engine yang memiliki tujuan untuk menampilkan data/informasi yang relevan dengan permintaan atau keyword yang dicari oleh user . Sementara tujuan dari tugas ini adalah mengekstrak informasi dari website target kemudian melakukan preprocessing text yang kemudian akan di seleksi fitur dan pada akhirnya data akan di cluster menjadi beberapa kelompok. Tools dan Environment Python 3.6 Database Kamus Besar Bahasa Indonesia Library untuk Crawling : BeautifulSoup4 , requests Library untuk Preprocessing Text : Sastrawi Library untuk Seleksi Fitur dan Clustering : numpy , scipy , sklearn , skfuzzy Library untuk menyimpan data : sqlite3 (database), csv (import data ke csv) Website target : https://www.codepolitan.com/articles Data yang diambil adalah data artikel dari 3 pages, dimana tiap pages memiliki 16 artikel, jadi total data yang diambil adalah 48 data. Running pertama kali program membutuhkan koneksi internet untuk mengambil data dari website, jika sudah memiliki data yang di inginkan maka tidak perlu update data di awal. Crawling Data Proses pertama yang dilakukan adalah crawling data . Seperti yang sudah dijelaskan diatas, proses ini adalah proses pengambilan data dari website target. Code crawling data : Koneksi ke library import requests from bs4 import BeautifulSoup Penghubung ke website src = \"https://www.codepolitan.com/articles\" page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') Variabel penghubung ke html linkhead = soup.findAll(class_='block') pagination = soup.find(class_='pagination-wrapper') pagenum = pagination.findAll('a') Variabel linkhead untuk mengambil data dari kelas seluruh class block yang berada di page, variabel ini nantinya akan berguna sebagai dasar untuk pengambilan data tiap artikel Variabel pagination dan pagenum digunakan untuk mengambil data class pagination-wrapper yang akan digunakan untuk perpindahan menuju page selanjutnya. List penampung untuk crawling next page numpage = [] for i in pagination: numpage.append(i.getText()) List ini digunakan untuk menampung data data dari variabel pagination yang nantinya salah satu index datanya akan digunakan untuk perpindahan menuju page selanjutnya. Looping untuk mengambil data dalam tiap artikel for links in linkhead: try : src = links['href'] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find('article') title = konten.find(class_='type--bold').getText() author = konten.find('a').getText() temp = konten.findAll('p') isi = [] for j in range(len(temp)): isi += [temp[j].getText()] isif = \"\" for i in isi: isif += i Pada bagian ini pengambilan data dilakukan, dimana data yang diambil adalah judul artikel, pengarang artikel, dan isi dari artikel itu sendiri. Pengambilan data dilakukan dengan inspect elemen html dari page website target. Lalu ambil tag html yang sesuai dengan data yang diinginkan. Variabel konten merupakan parents data penampung dari tiap data. Variabel title dan author untuk mengambil data dari judul dan penulis. Variabel temp untuk mengambil data isi artikel. Dimana isi artikel memiliki banyak paragraf yang akan ditampung dalam list isi, lalu dijadikan kembali menjadi satu dalam variabel isif. Menyimpan Data ke Database Setelah berhasil melakukan crawling data dari website, maka data hasil crawling tersebut akan kita simpan ke dalam database. Code penyimpan data ke database : import sqlite3 conn = sqlite3.connect('datacoba.sqlite') conn.execute('''CREATE TABLE if not exists ARTIKEL (TITLE TEXT NOT NULL, AUTHOR TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.execute(\"INSERT INTO ARTIKEL (TITLE, AUTHOR, ISI) VALUES (?, ?, ?)\", (title, author, isif)) conn.commit() Import Data ke CSV Selain disimpan ke dalam database, data hasil crawling dan proses yang lain juga akan di import menjadi file csv agar lebih mudah di akses. Disini kita akan membuat fungsi tersendiri untuk import data ke dalam csv karena nantinya akan digunakan berulang kali. Berikut code untuk import ke csv : def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) Preprocessing Text Preprocessing text merupakan tahapan dimana data crawling di seleksi kembali atau di normalisasi agar tersusun menjadi data data yang lebih terstruktur untuk di analisi kembali pada tahap selanjutnya. Tahapan preprocessing text adalah sebagai berikut : Tokenisasi, merupakan tahap dimana tiap kata dalam data dipisah atau dipecah menjadi index-index tersendiri. Stopword, tahap setelah tokenisasi selesai dilakukan, yaitu proses seleksi kata-kata hubung atau tanda baca yang tidak diperlukan dari tiap kata. Stemming, tahap penyusutan kata dengan cara mengambil nilai kata dasar dari tiap kata. Tambahan proses seleksi data dengan menggunakan database Kamus Besar Bahasa Indonesia dimana kata yang telah melalui proses stemming akan dicocokkan dengan data yang berada dalam database KBBI, jika kata tersebut ada maka akan disimpan, dan jika tidak maka akan dibuang. Code untuk preprocessing text : # preprocessing text factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() stop = stopword.remove(isif) stem = stemmer.stem(stop) katadasar = stem.split() matrix=[] for row in cursor: tampung = [] for i in katadasar: tampung.append(row[2].lower().count(i)) matrix.append(tampung) print(katadasar) write_csv(\"kata_before_%s.csv\"%n, katadasar) #Sharing kata Sesuai KBI Belum VSM conn = sqlite3.connect('KBI.db') cur_kbi = conn.execute(\"SELECT* from KATA\") # fungsi cek kata dengan KBBI def LinearSearch (kbi,kata): found=False posisi=0 while posisi < len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] berhasil2='' for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) berhasil2=berhasil2+' '+kata print(berhasil) #import csv kata yg sesuai dengan KBI write_csv(\"kata_after_%s.csv\"%n, berhasil) TF-IDF TF-IDF merupakan dua statistik data yang terdiri dari Term-Frequency dan Inverse Data-Frequency . TF(Term-Frequency) Merupakan data statistik yang berisi frekuensi kemunculan dari tiap kata (fitur) terhadap tiap data. data 1 : \"saya makan nasi\" data 2 : \"nasi putih di makan saya\" data saya makan nasi putih di 1 1 1 0 0 0 2 1 1 1 1 1 Code untuk TF : # VSM setelah cek KBI conn = sqlite3.connect('datacoba.sqlite') matrix2=[] cursor = conn.execute(\"SELECT* from ARTIKEL\") for row in cursor: tampung = [] for i in berhasil: tampung.append(row[2].lower().count(i)) matrix2.append(tampung) print(matrix2) IDF(Inverse Data-Frequency) Data statistik yang berisi hasil inverse dari Data-Frequency (DF) dimana DF didapatkan dari nilai kemunculan tiap kata (fitur) berapa kali dalam semua data. data 1 : \"saya makan nasi\" data 2 : \"nasi putih di makan saya\" kata jumlah data yang mengandung kata saya 2 makan 2 nasi 2 putih 1 di 1 Code untuk DF : df = list() for d in range (len(matrix2[0])): total = 0 for i in range(len(matrix2)): if matrix2[i][d] !=0: total += 1 df.append(total) Setelah mendapatkan nilai DF maka yang selanjutnya dilakukan adalah menghitung nilai invers nya ( IDF ). Code untuk menghitung IDF : idf = list() for i in df: tmp = 1 + log10(len(matrix2)/(1+i)) idf.append(tmp) tf = matrix2 tfidf = [] for baris in range(len(matrix2)): tampungBaris = [] for kolom in range(len(matrix2[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) # import csv data tdidf write_csv(\"tfidf_%s.csv\"%n, tfidf) Seleksi Fitur Proses reduksi dimensi dari fitur yang ada dengan menggunakan suatu tolak ukur untuk menentukan sesuai atau tidaknya fitur tersebut dengan kebutuhan proses analisis selanjutnya. Seleksi fitur memiliki banyak metode, salah satunya adalah Pearson Correlation Coefficient (PCC) yang akan kita gunakan dalam tugas kali ini. PCC menyeleksi fitur menggunakan perbandingan nilai koefisien antar tiap dua data. Semakin tinggi nilai koefisien korelasi fitur dengan koefisien batas yang ditentukan maka semakin kuat hubungan fiturny, begitu pula sebaliknya, semakin kecil nilai koefisien korelasi maka semakin lemah hubungan fiturnya. Fitur yang memiliki korelasi paling lemah akan dibuang/di drop dari data yang ada. Code untuk Pearson Selection : def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(data, threshold, berhasil): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] seleksikata=berhasil[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) seleksikata = np.hstack((seleksikata, berhasil[v])) v+=1 data = dataBaru meanFitur=meanBaru berhasil=seleksikata if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data, seleksikata xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil) xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil) # import csv data setelah di seleksi fitur write_csv(\"kata_pearson_%s.csv\"%n, kataBaru2) Clustering Merupakan proses pengelompokan data yang memiliki karakteristik sama menjadi satu kelompok yang sama. Metode untuk clustering ada banyak sekali. Namun kali ini kita akan menggunakan metode Fuzzy C-Mean . Dalam proses ini kita akan meng clusterisasi data menggunakan library dari python yaitu sklearn dan skfuzzy. Code untuk clustering : print(\"Cluster dgn Seleksi Fitur : 0.8\") cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) silhouette = silhouette_samples(xBaru1, membership) s_avg = silhouette_score(xBaru1, membership, random_state=10) for i in range(len(tfidf)): print(\"c \"+str(membership[i]))#+\"\\t\" + str(silhouette[i])) print(s_avg) fuzz.cmeans memiliki beberapa parameter utama, yang pertama parameter untuk menampung data matriks yang akan di cluster, parameter kedua adalah jumlah cluster yang ingin dibagi, yang ketiga adalah nilai m atau pembobot, parameter keempat untuk mengatur nilai error maksimal, lalu parameter kelima untuk mengatur jumlah maksimal iterasi.","title":"Web Crawling"},{"location":"#web-crawling","text":"Nama : Afni Sakinah NIM : 160411100077 Mata Kuliah : Penambangan dan Pencarian Web (Web ini dibuat untuk memenuhi tugas Ujian Tengah Semester mata kuliah Penambangan dan Pencarian Web serta untuk membagikan ilmu yang telah di dapat kepada khalayak umum)","title":"Web Crawling"},{"location":"#definisi","text":"Web Crawler merupakan sebuah program atau script yang menggunakan metode tertentu yang memiliki tujuan untuk mengumpulkan secara otomatis semua data/informasi yang berada pada suatu website. Proses dari web crawler mengunjungi tiap dokumen dari website ini lah yang disebut dengan Web Crawling .","title":"Definisi"},{"location":"#tujuan","text":"Tujuan utama dari web crawler adalah untuk mengumpulkan informasi-informasi yang ada dalam suatu halaman web. Contoh penggunaannya yang paling umum adalah search engine yang memiliki tujuan untuk menampilkan data/informasi yang relevan dengan permintaan atau keyword yang dicari oleh user . Sementara tujuan dari tugas ini adalah mengekstrak informasi dari website target kemudian melakukan preprocessing text yang kemudian akan di seleksi fitur dan pada akhirnya data akan di cluster menjadi beberapa kelompok.","title":"Tujuan"},{"location":"#tools-dan-environment","text":"Python 3.6 Database Kamus Besar Bahasa Indonesia Library untuk Crawling : BeautifulSoup4 , requests Library untuk Preprocessing Text : Sastrawi Library untuk Seleksi Fitur dan Clustering : numpy , scipy , sklearn , skfuzzy Library untuk menyimpan data : sqlite3 (database), csv (import data ke csv) Website target : https://www.codepolitan.com/articles Data yang diambil adalah data artikel dari 3 pages, dimana tiap pages memiliki 16 artikel, jadi total data yang diambil adalah 48 data. Running pertama kali program membutuhkan koneksi internet untuk mengambil data dari website, jika sudah memiliki data yang di inginkan maka tidak perlu update data di awal.","title":"Tools dan Environment"},{"location":"#crawling-data","text":"Proses pertama yang dilakukan adalah crawling data . Seperti yang sudah dijelaskan diatas, proses ini adalah proses pengambilan data dari website target. Code crawling data : Koneksi ke library import requests from bs4 import BeautifulSoup Penghubung ke website src = \"https://www.codepolitan.com/articles\" page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') Variabel penghubung ke html linkhead = soup.findAll(class_='block') pagination = soup.find(class_='pagination-wrapper') pagenum = pagination.findAll('a') Variabel linkhead untuk mengambil data dari kelas seluruh class block yang berada di page, variabel ini nantinya akan berguna sebagai dasar untuk pengambilan data tiap artikel Variabel pagination dan pagenum digunakan untuk mengambil data class pagination-wrapper yang akan digunakan untuk perpindahan menuju page selanjutnya. List penampung untuk crawling next page numpage = [] for i in pagination: numpage.append(i.getText()) List ini digunakan untuk menampung data data dari variabel pagination yang nantinya salah satu index datanya akan digunakan untuk perpindahan menuju page selanjutnya. Looping untuk mengambil data dalam tiap artikel for links in linkhead: try : src = links['href'] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find('article') title = konten.find(class_='type--bold').getText() author = konten.find('a').getText() temp = konten.findAll('p') isi = [] for j in range(len(temp)): isi += [temp[j].getText()] isif = \"\" for i in isi: isif += i Pada bagian ini pengambilan data dilakukan, dimana data yang diambil adalah judul artikel, pengarang artikel, dan isi dari artikel itu sendiri. Pengambilan data dilakukan dengan inspect elemen html dari page website target. Lalu ambil tag html yang sesuai dengan data yang diinginkan. Variabel konten merupakan parents data penampung dari tiap data. Variabel title dan author untuk mengambil data dari judul dan penulis. Variabel temp untuk mengambil data isi artikel. Dimana isi artikel memiliki banyak paragraf yang akan ditampung dalam list isi, lalu dijadikan kembali menjadi satu dalam variabel isif.","title":"Crawling Data"},{"location":"#menyimpan-data-ke-database","text":"Setelah berhasil melakukan crawling data dari website, maka data hasil crawling tersebut akan kita simpan ke dalam database. Code penyimpan data ke database : import sqlite3 conn = sqlite3.connect('datacoba.sqlite') conn.execute('''CREATE TABLE if not exists ARTIKEL (TITLE TEXT NOT NULL, AUTHOR TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.execute(\"INSERT INTO ARTIKEL (TITLE, AUTHOR, ISI) VALUES (?, ?, ?)\", (title, author, isif)) conn.commit()","title":"Menyimpan Data ke Database"},{"location":"#import-data-ke-csv","text":"Selain disimpan ke dalam database, data hasil crawling dan proses yang lain juga akan di import menjadi file csv agar lebih mudah di akses. Disini kita akan membuat fungsi tersendiri untuk import data ke dalam csv karena nantinya akan digunakan berulang kali. Berikut code untuk import ke csv : def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row)","title":"Import Data ke CSV"},{"location":"#preprocessing-text","text":"Preprocessing text merupakan tahapan dimana data crawling di seleksi kembali atau di normalisasi agar tersusun menjadi data data yang lebih terstruktur untuk di analisi kembali pada tahap selanjutnya. Tahapan preprocessing text adalah sebagai berikut : Tokenisasi, merupakan tahap dimana tiap kata dalam data dipisah atau dipecah menjadi index-index tersendiri. Stopword, tahap setelah tokenisasi selesai dilakukan, yaitu proses seleksi kata-kata hubung atau tanda baca yang tidak diperlukan dari tiap kata. Stemming, tahap penyusutan kata dengan cara mengambil nilai kata dasar dari tiap kata. Tambahan proses seleksi data dengan menggunakan database Kamus Besar Bahasa Indonesia dimana kata yang telah melalui proses stemming akan dicocokkan dengan data yang berada dalam database KBBI, jika kata tersebut ada maka akan disimpan, dan jika tidak maka akan dibuang. Code untuk preprocessing text : # preprocessing text factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() stop = stopword.remove(isif) stem = stemmer.stem(stop) katadasar = stem.split() matrix=[] for row in cursor: tampung = [] for i in katadasar: tampung.append(row[2].lower().count(i)) matrix.append(tampung) print(katadasar) write_csv(\"kata_before_%s.csv\"%n, katadasar) #Sharing kata Sesuai KBI Belum VSM conn = sqlite3.connect('KBI.db') cur_kbi = conn.execute(\"SELECT* from KATA\") # fungsi cek kata dengan KBBI def LinearSearch (kbi,kata): found=False posisi=0 while posisi < len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] berhasil2='' for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) berhasil2=berhasil2+' '+kata print(berhasil) #import csv kata yg sesuai dengan KBI write_csv(\"kata_after_%s.csv\"%n, berhasil)","title":"Preprocessing Text"},{"location":"#tf-idf","text":"TF-IDF merupakan dua statistik data yang terdiri dari Term-Frequency dan Inverse Data-Frequency .","title":"TF-IDF"},{"location":"#tfterm-frequency","text":"Merupakan data statistik yang berisi frekuensi kemunculan dari tiap kata (fitur) terhadap tiap data. data 1 : \"saya makan nasi\" data 2 : \"nasi putih di makan saya\" data saya makan nasi putih di 1 1 1 0 0 0 2 1 1 1 1 1 Code untuk TF : # VSM setelah cek KBI conn = sqlite3.connect('datacoba.sqlite') matrix2=[] cursor = conn.execute(\"SELECT* from ARTIKEL\") for row in cursor: tampung = [] for i in berhasil: tampung.append(row[2].lower().count(i)) matrix2.append(tampung) print(matrix2)","title":"TF(Term-Frequency)"},{"location":"#idfinverse-data-frequency","text":"Data statistik yang berisi hasil inverse dari Data-Frequency (DF) dimana DF didapatkan dari nilai kemunculan tiap kata (fitur) berapa kali dalam semua data. data 1 : \"saya makan nasi\" data 2 : \"nasi putih di makan saya\" kata jumlah data yang mengandung kata saya 2 makan 2 nasi 2 putih 1 di 1 Code untuk DF : df = list() for d in range (len(matrix2[0])): total = 0 for i in range(len(matrix2)): if matrix2[i][d] !=0: total += 1 df.append(total) Setelah mendapatkan nilai DF maka yang selanjutnya dilakukan adalah menghitung nilai invers nya ( IDF ). Code untuk menghitung IDF : idf = list() for i in df: tmp = 1 + log10(len(matrix2)/(1+i)) idf.append(tmp) tf = matrix2 tfidf = [] for baris in range(len(matrix2)): tampungBaris = [] for kolom in range(len(matrix2[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) # import csv data tdidf write_csv(\"tfidf_%s.csv\"%n, tfidf)","title":"IDF(Inverse Data-Frequency)"},{"location":"#seleksi-fitur","text":"Proses reduksi dimensi dari fitur yang ada dengan menggunakan suatu tolak ukur untuk menentukan sesuai atau tidaknya fitur tersebut dengan kebutuhan proses analisis selanjutnya. Seleksi fitur memiliki banyak metode, salah satunya adalah Pearson Correlation Coefficient (PCC) yang akan kita gunakan dalam tugas kali ini. PCC menyeleksi fitur menggunakan perbandingan nilai koefisien antar tiap dua data. Semakin tinggi nilai koefisien korelasi fitur dengan koefisien batas yang ditentukan maka semakin kuat hubungan fiturny, begitu pula sebaliknya, semakin kecil nilai koefisien korelasi maka semakin lemah hubungan fiturnya. Fitur yang memiliki korelasi paling lemah akan dibuang/di drop dari data yang ada. Code untuk Pearson Selection : def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(data, threshold, berhasil): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] seleksikata=berhasil[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) seleksikata = np.hstack((seleksikata, berhasil[v])) v+=1 data = dataBaru meanFitur=meanBaru berhasil=seleksikata if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data, seleksikata xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil) xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil) # import csv data setelah di seleksi fitur write_csv(\"kata_pearson_%s.csv\"%n, kataBaru2)","title":"Seleksi Fitur"},{"location":"#clustering","text":"Merupakan proses pengelompokan data yang memiliki karakteristik sama menjadi satu kelompok yang sama. Metode untuk clustering ada banyak sekali. Namun kali ini kita akan menggunakan metode Fuzzy C-Mean . Dalam proses ini kita akan meng clusterisasi data menggunakan library dari python yaitu sklearn dan skfuzzy. Code untuk clustering : print(\"Cluster dgn Seleksi Fitur : 0.8\") cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) silhouette = silhouette_samples(xBaru1, membership) s_avg = silhouette_score(xBaru1, membership, random_state=10) for i in range(len(tfidf)): print(\"c \"+str(membership[i]))#+\"\\t\" + str(silhouette[i])) print(s_avg) fuzz.cmeans memiliki beberapa parameter utama, yang pertama parameter untuk menampung data matriks yang akan di cluster, parameter kedua adalah jumlah cluster yang ingin dibagi, yang ketiga adalah nilai m atau pembobot, parameter keempat untuk mengatur nilai error maksimal, lalu parameter kelima untuk mengatur jumlah maksimal iterasi.","title":"Clustering"}]}