<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="None">
        
        
        <link rel="shortcut icon" href="img/favicon.ico">
        <title>My Docs</title>
        <link href="css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="css/font-awesome.min.css" rel="stylesheet">
        <link href="css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="js/jquery-1.10.2.min.js" defer></script>
        <script src="js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body class="homepage">

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <a class="navbar-brand" href=".">My Docs</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#web-crawling">Web Crawling</a></li>
            <li><a href="#definisi">Definisi</a></li>
            <li><a href="#tujuan">Tujuan</a></li>
            <li><a href="#tools-dan-environment">Tools dan Environment</a></li>
            <li><a href="#crawling-data">Crawling Data</a></li>
            <li><a href="#menyimpan-data-ke-database">Menyimpan Data ke Database</a></li>
            <li><a href="#import-data-ke-csv">Import Data ke CSV</a></li>
            <li><a href="#preprocessing-text">Preprocessing Text</a></li>
            <li><a href="#tf-idf">TF-IDF</a></li>
            <li><a href="#seleksi-fitur">Seleksi Fitur</a></li>
            <li><a href="#clustering">Clustering</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="web-crawling">Web Crawling</h1>
<p>Nama : Afni Sakinah</p>
<p>NIM : 160411100077</p>
<p>Mata Kuliah : Penambangan dan Pencarian Web</p>
<p>(Web ini dibuat untuk memenuhi tugas Ujian Tengah Semester mata kuliah Penambangan dan Pencarian Web serta untuk membagikan ilmu yang telah di dapat kepada khalayak umum)</p>
<h4 id="definisi">Definisi</h4>
<p><em>Web Crawler</em> merupakan sebuah program atau script yang menggunakan metode tertentu yang memiliki tujuan untuk mengumpulkan secara otomatis semua data/informasi yang berada pada suatu website. Proses dari <em>web crawler</em> mengunjungi tiap dokumen dari website ini lah yang disebut dengan <em>Web Crawling</em>.</p>
<h4 id="tujuan">Tujuan</h4>
<p>Tujuan utama dari <em>web crawler</em> adalah untuk mengumpulkan informasi-informasi yang ada dalam suatu halaman web. Contoh penggunaannya yang paling umum adalah <em>search engine</em> yang memiliki tujuan untuk menampilkan data/informasi yang relevan dengan permintaan atau <em>keyword</em> yang dicari oleh <em>user</em>.</p>
<p>Sementara tujuan dari tugas ini adalah mengekstrak informasi dari website target kemudian melakukan preprocessing text yang kemudian akan di seleksi fitur dan pada akhirnya data akan di cluster menjadi beberapa kelompok.</p>
<h2 id="tools-dan-environment">Tools dan Environment</h2>
<ul>
<li><code>Python 3.6</code></li>
<li>Database Kamus Besar Bahasa Indonesia</li>
<li>
<p>Library untuk Crawling : <code>BeautifulSoup4</code>, <code>requests</code></p>
</li>
<li>
<p>Library untuk Preprocessing Text : <code>Sastrawi</code></p>
</li>
<li>
<p>Library untuk Seleksi Fitur dan Clustering : <code>numpy</code>, <code>scipy</code>, <code>sklearn</code>, <code>skfuzzy</code></p>
</li>
<li>
<p>Library untuk menyimpan data : <code>sqlite3</code> (database), <code>csv</code> (import data ke csv)</p>
</li>
<li>
<p>Website target : <a href="https://www.codepolitan.com/articles">https://www.codepolitan.com/articles</a></p>
</li>
<li>Data yang diambil adalah data artikel dari 3 pages, dimana tiap pages memiliki 16 artikel, jadi total data yang diambil adalah 48 data.</li>
<li>Running pertama kali program membutuhkan koneksi internet untuk mengambil data dari website, jika sudah memiliki data yang di inginkan maka tidak perlu update data di awal.</li>
</ul>
<h2 id="crawling-data">Crawling Data</h2>
<p>Proses pertama yang dilakukan adalah <em>crawling data</em>. Seperti yang sudah dijelaskan diatas, proses ini adalah proses pengambilan data dari website target.</p>
<p>Code crawling data :</p>
<p>Koneksi ke library</p>
<pre><code>import requests
from bs4 import BeautifulSoup
</code></pre>

<p>Penghubung ke website </p>
<pre><code>src = &quot;https://www.codepolitan.com/articles&quot;
page = requests.get(src)
soup = BeautifulSoup(page.content, 'html.parser')
</code></pre>

<p>Variabel penghubung ke html </p>
<pre><code>linkhead = soup.findAll(class_='block')
pagination = soup.find(class_='pagination-wrapper')
pagenum = pagination.findAll('a')
</code></pre>

<blockquote>
<p>Variabel linkhead untuk mengambil data dari kelas seluruh class block yang berada di page, variabel ini nantinya akan berguna sebagai dasar untuk pengambilan data tiap artikel</p>
<p>Variabel pagination dan pagenum digunakan untuk mengambil data class pagination-wrapper yang akan digunakan untuk perpindahan menuju page selanjutnya.</p>
</blockquote>
<p>List penampung untuk crawling next page</p>
<pre><code>numpage = []
for i in pagination:
        numpage.append(i.getText())
</code></pre>

<blockquote>
<p>List ini digunakan untuk menampung data data dari variabel pagination yang nantinya salah satu index datanya akan digunakan untuk perpindahan menuju page selanjutnya.</p>
</blockquote>
<p>Looping untuk mengambil data dalam tiap artikel</p>
<pre><code>for links in linkhead:
            try :
                src = links['href']
                page = requests.get(src)
                soup = BeautifulSoup(page.content, 'html.parser')

                konten = soup.find('article')
                title = konten.find(class_='type--bold').getText()
                author = konten.find('a').getText()

                temp = konten.findAll('p')

                isi = []
                for j in range(len(temp)):
                    isi += [temp[j].getText()]

                isif = &quot;&quot;
                for i in isi:
                    isif += i
</code></pre>

<blockquote>
<p>Pada bagian ini pengambilan data dilakukan, dimana data yang diambil adalah judul artikel, pengarang artikel, dan isi dari artikel itu sendiri.</p>
<p>Pengambilan data dilakukan dengan inspect elemen html dari page website target. Lalu ambil tag html yang sesuai dengan data yang diinginkan.</p>
<p>Variabel konten merupakan parents data penampung dari tiap data.</p>
<p>Variabel title dan author untuk mengambil data dari judul dan penulis.</p>
<p>Variabel temp untuk mengambil data isi artikel. Dimana isi artikel memiliki banyak paragraf yang akan ditampung dalam list isi, lalu dijadikan kembali menjadi satu dalam variabel isif.</p>
</blockquote>
<h2 id="menyimpan-data-ke-database">Menyimpan Data ke Database</h2>
<p>Setelah berhasil melakukan <em>crawling data</em> dari website, maka data hasil <em>crawling</em> tersebut akan kita simpan ke dalam database.</p>
<p>Code penyimpan data ke database :</p>
<pre><code>import sqlite3
conn = sqlite3.connect('datacoba.sqlite')
conn.execute('''CREATE TABLE if not exists ARTIKEL
                (TITLE         TEXT     NOT NULL,
                 AUTHOR         TEXT     NOT NULL,
                 ISI         TEXT     NOT NULL);''')
conn.execute(&quot;INSERT INTO ARTIKEL (TITLE, AUTHOR, ISI) 
            VALUES (?, ?, ?)&quot;, (title, author, isif))
conn.commit()
</code></pre>

<h2 id="import-data-ke-csv">Import Data ke CSV</h2>
<p>Selain disimpan ke dalam database, data hasil crawling dan proses yang lain juga akan di import menjadi file csv agar lebih mudah di akses.</p>
<p>Disini kita akan membuat fungsi tersendiri untuk import data ke dalam csv karena nantinya akan digunakan berulang kali.</p>
<p>Berikut code untuk import ke csv :</p>
<pre><code>def write_csv(nama_file, isi, tipe='w'):
    'tipe=w; write; tipe=a; append;'
    with open(nama_file, mode=tipe) as tbl:
    tbl_writer = csv.writer(tbl, delimiter=',', quotechar='&quot;', quoting=csv.QUOTE_MINIMAL)
        for row in isi:
            tbl_writer.writerow(row)
</code></pre>

<h2 id="preprocessing-text">Preprocessing Text</h2>
<p>Preprocessing text merupakan tahapan dimana data <em>crawling</em> di seleksi kembali atau di normalisasi agar tersusun menjadi data data yang lebih terstruktur untuk di analisi kembali pada tahap selanjutnya.</p>
<p>Tahapan preprocessing text adalah sebagai berikut :</p>
<ol>
<li>Tokenisasi, merupakan tahap dimana tiap kata dalam data dipisah atau dipecah menjadi index-index tersendiri. </li>
<li>Stopword, tahap setelah tokenisasi selesai dilakukan, yaitu proses seleksi kata-kata hubung atau tanda baca yang tidak diperlukan dari tiap kata.</li>
<li>Stemming, tahap penyusutan kata dengan cara mengambil nilai kata dasar dari tiap kata.</li>
<li>Tambahan proses seleksi data dengan menggunakan database Kamus Besar Bahasa Indonesia dimana kata yang telah melalui proses stemming akan dicocokkan dengan data yang berada dalam database KBBI, jika kata tersebut ada maka akan disimpan, dan jika tidak maka akan dibuang.</li>
</ol>
<p>Code untuk preprocessing text :</p>
<pre><code># preprocessing text
factory = StopWordRemoverFactory() 
stopword = factory.create_stop_word_remover()

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
factory = StemmerFactory()
stemmer = factory.create_stemmer()
stop = stopword.remove(isif)
stem = stemmer.stem(stop)
katadasar = stem.split()

matrix=[]
for row in cursor:
    tampung = []
    for i in katadasar:
        tampung.append(row[2].lower().count(i))
    matrix.append(tampung)

print(katadasar)

write_csv(&quot;kata_before_%s.csv&quot;%n, katadasar)

#Sharing kata Sesuai KBI Belum VSM
conn = sqlite3.connect('KBI.db')
cur_kbi = conn.execute(&quot;SELECT* from KATA&quot;)

# fungsi cek kata dengan KBBI
def LinearSearch (kbi,kata):
    found=False
    posisi=0
    while posisi &lt; len (kata) and not found :
        if kata[posisi]==kbi:
            found=True
        posisi=posisi+1
    return found

berhasil=[]
berhasil2=''
for kata in cur_kbi :
    ketemu=LinearSearch(kata[0],katadasar)
    if ketemu :
        kata = kata[0]
        berhasil.append(kata)
        berhasil2=berhasil2+' '+kata
print(berhasil)

#import csv kata yg sesuai dengan KBI
write_csv(&quot;kata_after_%s.csv&quot;%n, berhasil)
</code></pre>

<h2 id="tf-idf">TF-IDF</h2>
<p>TF-IDF merupakan dua statistik data yang terdiri dari <em>Term-Frequency</em> dan <em>Inverse Data-Frequency</em>.</p>
<h5 id="tfterm-frequency">TF(Term-Frequency)</h5>
<p>Merupakan data statistik yang berisi frekuensi kemunculan dari tiap kata (fitur) terhadap tiap data.</p>
<p>data 1 : "saya makan nasi"</p>
<p>data 2 : "nasi putih di makan saya"</p>
<table>
<thead>
<tr>
<th>data</th>
<th>saya</th>
<th>makan</th>
<th>nasi</th>
<th>putih</th>
<th>di</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Code untuk <em>TF</em> :</p>
<pre><code># VSM setelah cek KBI
conn = sqlite3.connect('datacoba.sqlite')
matrix2=[]
cursor = conn.execute(&quot;SELECT* from ARTIKEL&quot;)
for row in cursor:
    tampung = []
    for i in berhasil:
        tampung.append(row[2].lower().count(i))
    matrix2.append(tampung)
print(matrix2)
</code></pre>

<h5 id="idfinverse-data-frequency">IDF(Inverse Data-Frequency)</h5>
<p>Data statistik yang berisi hasil inverse dari <em>Data-Frequency (DF)</em> dimana <em>DF</em> didapatkan dari nilai kemunculan tiap kata (fitur) berapa kali dalam semua data.</p>
<p>data 1 : "saya makan nasi"</p>
<p>data 2 : "nasi putih di makan saya"</p>
<table>
<thead>
<tr>
<th>kata</th>
<th>jumlah data yang mengandung kata</th>
</tr>
</thead>
<tbody>
<tr>
<td>saya</td>
<td>2</td>
</tr>
<tr>
<td>makan</td>
<td>2</td>
</tr>
<tr>
<td>nasi</td>
<td>2</td>
</tr>
<tr>
<td>putih</td>
<td>1</td>
</tr>
<tr>
<td>di</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Code untuk <em>DF</em> :</p>
<pre><code>df = list()
for d in range (len(matrix2[0])):
    total = 0
    for i in range(len(matrix2)):
        if matrix2[i][d] !=0:
            total += 1
    df.append(total)
</code></pre>

<p>Setelah mendapatkan nilai <em>DF</em> maka yang selanjutnya dilakukan adalah menghitung nilai invers nya (<em>IDF</em>).</p>
<p>Code untuk menghitung <em>IDF</em>:</p>
<pre><code>idf = list()
for i in df:
    tmp = 1 + log10(len(matrix2)/(1+i))
    idf.append(tmp)

tf = matrix2
tfidf = []
for baris in range(len(matrix2)):
    tampungBaris = []
    for kolom in range(len(matrix2[0])):
        tmp = tf[baris][kolom] * idf[kolom]
        tampungBaris.append(tmp)
    tfidf.append(tampungBaris)

# import csv data tdidf
write_csv(&quot;tfidf_%s.csv&quot;%n, tfidf)
</code></pre>

<h2 id="seleksi-fitur">Seleksi Fitur</h2>
<p>Proses reduksi dimensi dari fitur yang ada dengan menggunakan suatu tolak ukur untuk menentukan sesuai atau tidaknya fitur tersebut dengan kebutuhan proses analisis selanjutnya.</p>
<p>Seleksi fitur memiliki banyak metode, salah satunya adalah <em>Pearson Correlation Coefficient (PCC)</em> yang akan kita gunakan dalam tugas kali ini.</p>
<p><em>PCC</em> menyeleksi  fitur menggunakan perbandingan nilai koefisien antar tiap dua data. Semakin tinggi nilai koefisien korelasi fitur dengan koefisien batas yang ditentukan maka semakin kuat hubungan fiturny, begitu pula sebaliknya, semakin kecil nilai koefisien korelasi maka semakin lemah hubungan fiturnya. Fitur yang memiliki korelasi paling lemah akan dibuang/di<em>drop</em> dari data yang ada.</p>
<p>Code untuk Pearson Selection :</p>
<pre><code>def pearsonCalculate(data, u,v):
    &quot;i, j is an index&quot;
    atas=0; bawah_kiri=0; bawah_kanan = 0
    for k in range(len(data)):
        atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v])
        bawah_kiri += (data[k,u] - meanFitur[u])**2
        bawah_kanan += (data[k,v] - meanFitur[v])**2
    bawah_kiri = bawah_kiri ** 0.5
    bawah_kanan = bawah_kanan ** 0.5
    return atas/(bawah_kiri * bawah_kanan)
def meanF(data):
    meanFitur=[]
    for i in range(len(data[0])):
        meanFitur.append(sum(data[:,i])/len(data))
    return np.array(meanFitur)
def seleksiFiturPearson(data, threshold, berhasil):
    global meanFitur
    data = np.array(data)
    meanFitur = meanF(data)
    u=0
    while u &lt; len(data[0]):
        dataBaru=data[:, :u+1]
        meanBaru=meanFitur[:u+1]
        seleksikata=berhasil[:u+1]
        v = u
        while v &lt; len(data[0]):
            if u != v:
                value = pearsonCalculate(data, u,v)
                if value &lt; threshold:
                    dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1)))
                    meanBaru = np.hstack((meanBaru, meanFitur[v]))
                    seleksikata = np.hstack((seleksikata, berhasil[v]))
            v+=1
        data = dataBaru
        meanFitur=meanBaru
        berhasil=seleksikata
        if u%50 == 0 : print(&quot;proses : &quot;, u, data.shape)
        u+=1
    return data, seleksikata

xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil)
xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil)

# import csv data setelah di seleksi fitur
write_csv(&quot;kata_pearson_%s.csv&quot;%n, kataBaru2)
</code></pre>

<h2 id="clustering">Clustering</h2>
<p>Merupakan proses pengelompokan data yang memiliki karakteristik sama menjadi satu kelompok yang sama. Metode untuk <em>clustering</em> ada banyak sekali. Namun kali ini kita akan menggunakan metode <em>Fuzzy C-Mean</em>.</p>
<p>Dalam proses ini kita akan meng<em>clusterisasi</em> data menggunakan library dari python yaitu sklearn dan skfuzzy. </p>
<p>Code untuk clustering :</p>
<pre><code>print(&quot;Cluster dgn Seleksi Fitur : 0.8&quot;)
cntr, u, u0, distant, fObj, iterasi, fpc =  fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0)
membership = np.argmax(u, axis=0)

silhouette = silhouette_samples(xBaru1, membership)
s_avg = silhouette_score(xBaru1, membership, random_state=10)

for i in range(len(tfidf)):
    print(&quot;c &quot;+str(membership[i]))#+&quot;\t&quot; + str(silhouette[i]))
print(s_avg)

</code></pre>

<ul>
<li>fuzz.cmeans memiliki beberapa parameter utama, yang pertama parameter untuk menampung data matriks yang akan di cluster, parameter kedua adalah jumlah cluster yang ingin dibagi, yang ketiga adalah nilai m atau pembobot, parameter keempat untuk mengatur nilai error maksimal, lalu parameter kelima untuk mengatur jumlah maksimal iterasi.</li>
</ul></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = ".",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="js/base.js" defer></script>
        <script src="search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>

<!--
MkDocs version : 1.0.4
Build Date UTC : 2019-04-22 05:49:43
-->
